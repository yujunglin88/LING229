{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LING229 Final Project\n",
    "Jeff Lin, 300 599 323"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducion and Research Question\n",
    "\n",
    "As my background growing up is heavely influensed by music; Having studied music as my major for almost my entire upbringing, including my previous degree, I tend to pay more focus and attention to music adjacent topics. What has sparked interest in this project is the age-old debate in the music scene of: is the modern music of pop culture dumbing down the vocabulary in both music, and lyrics, whilst the music harmony side is not on the topic of discussion for this topic, the lyrics absolutely are!\n",
    "\n",
    "What I am interested in finding out via the project are quite simple on the paper: is the modern pop cultures (Billboard hits, etc) music dumbed down compared to other more \"tradtitioanl\" formats of music, such as musicals / opera. I will be attempting to analyse this through a number of matrics, laxical diversity, overlapping language usages, the usage of more comlex word spaces, etc.\n",
    "\n",
    "I am hoping that with the anlysise, I can come to a much broadened understanding of the difference in these genera and how it may / may not have changed, and maybe see some other intersting observations along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scourse\n",
    "\n",
    "The first stage of this project was to establise the genera we are working with, as mentioned, we are comparing pop culture to other genera, which, wich must be a genera of music which includes singing! In general, we have the following options: musical, opera, jazz/blues, religious, baroque, etc. Whilst these aren't the extensive list, it does represent the majority of the most popular genera over the times. \n",
    "\n",
    "For the selection process, first, jazz/blues are eliminated, as most jazz/blues singing includes a heavy portion / emphasis on scatting, which is a form of singing without meaningful lyrics, using instead nonsense syllables, vocables, or no words at all to create melodies and rhythms. This is not to say jazz/blues singing is only using scatting, but it is a major influce and technique used, thus it is my belif that the lyrics are not the focus of this genera, bringing not much value to this project's analysis. Religious and baroque music are then also removed, as through my study with music, it is generally understood that baroque music are heavely under the influence of regligion, and thus, have a built in baise of words strongly related to religous theme. As it is mostly just used to worship God, I believe it will also interfare with the ressearch, as the language useage won't nessasery be more/less complex, but rather just heavely baised towards religous text. Thus we settle with the uses of musical and opera as our comparison, and modern pop music as our control.\n",
    "\n",
    "As we are working with music lyrics, and as my main focus is on the modern pop culture, I thought that using Billboard Top Hits is the most suitable refercing point, as it ensentially reflects what the population in large has chosen to be their favrourits in the pop culture industry. For musicals, I searched for the top grossing Broadway musicals, as Broadway musicals are often is not always considered as where the best / most popular musicals are. for Opera, I serached for the most popular operas, and used the ones where it is most well known to the general public and uses Enligh, or has English translations.\n",
    "\n",
    "For the specific data source, are as follow:\n",
    "\n",
    "#### The list of sourse materials are consulted from the following websites: \n",
    "* [Official Charts](https://www.officialcharts.com/chart-news/the-official-biggest-songs-of-2024/)\n",
    "* [Broadway highest-grossing musicals](https://en.wikipedia.org/wiki/List_of_highest-grossing_musical_theatre_productions)\n",
    "* [Top Opera List](https://www.eno.org/discover-opera/explore-more/beginners-guide-to-famous-operas/)\n",
    "\n",
    "#### The lyrcis of chosen pieces are extracted from the following sites:\n",
    "* [Google Lyrics (for Billboard songs)](https://www.google.com)\n",
    "* [Musical Lyrics](https://www.allmusicals.com/)\n",
    "* [Comic Opera Guild](https://comicoperaguild.org/)\n",
    "* [Genius](https://genius.com/)\n",
    "* [Opera Guide](https://opera-guide.ch/en/)\n",
    "\n",
    "#### English Word frequency list:\n",
    "* [Wiktionary:Frequency lists/English/Project Gutenberg](https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/English/Project_Gutenberg)\n",
    "\n",
    "#### All corpus text and other files:\n",
    "* [My Github page](https://github.com/yujunglin88/LING229)\n",
    "\n",
    "The extraced lyrics are sorted into the three repective corpus folder as: Biillboard, Broaway, Opera. With Billboard, as the musical are generally quite short, to compensate for this I've included all of the top 40 pieces, which altogether should have sufficient data cummulated. For Broadway, I've chosen to include the following four: Book of Mormon, Hamilton, Lion King, and Phantom of Opera. Each of the opera includes averaging 20-40 pieces, which should also be enough to work with. For Opera, I've included the following three: Libretto: La Boh√®me, The Magic Flute, The Ring of Nibulung. Each Opera also have around 20-40 pieces each, though this time I've concated them into a single file, as we are not interesed in individule pieces but rather looking at a genera as collective, this won't be much of an issue. This means all together, the individule piece counts sits around 40, 100, 60 for Billboard, Broadway, and Opera, which should be more than enough to conduct the research.\n",
    "\n",
    "The structures of the corpa are a follows:\n",
    "\n",
    "```\n",
    "---corpus                           \n",
    "        ---Billboard                \n",
    "                ---2024             \n",
    "        ---Broadway                 \n",
    "                ---Combined         \n",
    "                ---Book of Mormon   \n",
    "                ---Hamilton         \n",
    "                ---Lion King        \n",
    "        ---Opera                    \n",
    "                ---Combined         \n",
    "```\n",
    "\n",
    "Where it is first categorised by the 3 main genera, then Billboard is sorted into years to allow for expantion to further this project by potentially including more years with existing structure. Broadways are sorted into each individule shows, and each of the pieces in the shows are stored individully. In both Broadway and Opera, I've also created a `Combined` subtag, this is for storing the shows's songs as one complete `.txt` file, where every piece in the show is combined into a single file.\n",
    "\n",
    "I have considered to re-structure the Broadways corpus to be like Opera, where it is concated into single .txt files and stored directly under the Broadway tag, but as it won't effect the outcome of this research I've decided to just leave it as it is.\n",
    "\n",
    "All lyrics are saved in .txt files, where for Billboard hits, it includes ONLY the lyrics and nothing else, so no special pre-processing is required. For Broadways and Operas, as the lyrics includes informaion of singers, I've gone ahead and made sure all singer cues are in capitalised format, which will first be filtered during this project, but left in the original text intentially in case it is nedded later down the line.\n",
    "\n",
    "All corpus will be loaded into the project as NLTK courpus using my custom built loader: [corpus_loader.py](../corpus_loader.py), it simply assists in scanning the given corpus dir and loads all `.txt` files into the NLKT corpus format, along with Dataframe and Dictionary formats in case it is required for some different actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Program\n",
    "\n",
    "My program can be simplified into three parts, where it first loads in all nessasery data and pre-processing, then processeds to look to basic metrics and visualisaion, finally moving to identifying overlapped / unique words to each category, and finding out the more complex words to each.\n",
    "\n",
    "#### Part 1: Data loadout + pre processing\n",
    "In this stage, I first ensure all necessary imports are loaded in properly, which includes installing missing ones. After all library imports are completed and constats setup, I start loading in all of the corpuses as `Billboard`, `Broadway`, and `Opera`, this is done by using my custom built `corpus_loader.py` which uses `os` calls to check on dirctories given to see matching files with `.txt` extensions, it then reads it all in as a NLTK corpus using NLTK's `PlaintextCorpusReader()`.\n",
    "\n",
    "When all corpuses have been loaded in, we also load in the word frequency list from the corpus dir, this file is then built into a dictionary object with `key: value` pairs as `word: ranking`, this allows me to do quick look-ups that points dirctly to a ranking bracket, which is useful in the part 3 of this project.\n",
    "\n",
    "The last thing to do now is to do pre-processing on the corpuses to minimise noise during the analysis phase, this is done by building a `WordProcessor` class, which takes in a string text to be processed and tokenised. it utlises many of NLTK's built in functions such as `word_tokenize()`, `WordNetLemmatizer()`, `RegexpTokenizer()`, `FreqDist()`, and `stopwords`, as the text files are quite clean to begin with. The only manual cleanups' required was to remove the full uppcase words, which are character ques/hints present in the `Broadway` and `Opera` corpus. In this stage I've kept two different version of the processed tokens, one set with stop words left in, and one set with stop words removed. This is deliberate as later on in the process I used both to compare the difference and to see the effects on overlapping and unique words to each corpus.\n",
    "\n",
    "Final action in this part is then simply to call on this class and process all corpuses and make sure it is ready to be used for the next parts.\n",
    "\n",
    "#### Part 2: basic data metrics and visualisation\n",
    "In this part, we first went through some basic metrics to get a gauge on what the corpuse is like, to make it easier to read and process, I put all calculated values into a pandas dataframe object and exported it as markdown tables, this not only lines up everything nicely for legibility but also allows for easy copy-pasting in case it is needed for reports elsewhere. The metrics is then printed out to console, alongside with a glimpse of what the word list looks like.\n",
    "\n",
    "The second part to this section is to then start visualise the word frequency lists, as during this stage we are starting to explore the differences between corpuses and stop words, both version is printed out and presented as a graph, which clearly showcase the relation before-after stopword removal, and its impact on the corpus.\n",
    "\n",
    "As there isn't much more other basic metrics I am interested or know of value to this project, it is then time to move on to the next stage, which explores into the more interesting parts of these corpuses.\n",
    "\n",
    "#### Part 3: Checking overlapping and unique words to each category and pairwise\n",
    "In the final sections of this program, I wanted to explore the similarities and difference of each corpus, and see if there are more complex / interesting changes across the genere, or if otherwise. \n",
    "\n",
    "I first started off by making word sets on each of the 3 corpuses, with each of its 2 variations, we then can utlise pythons buuilt in `intersection()` and `difference()` functions provided by `sets`, this allows me to very eaisly and quickly create sub-lists which are just the overlapping and/or unique words to each corpus, whether it is pair-wise of 3-venn.\n",
    "\n",
    "I first wanted to explore the intersections and differences of all 3 corpuses, so I used `matplotlib_venn`'s `venn3()` diagram to visualise the numbers. As the number of starting words is different across each corpus, I also made a duplicate version of the diagrame below the initial one, but presenting as percentages normalised across the effecting corpuses, this allows me to more easily comprehend the numbers and its relasions.\n",
    "\n",
    "After the 3-venn diagram, I also wanted to see and explore the pair-wise relations between a three corpuses, to which I then used the `venn2()` diagram, again with two version, numbers on first row and percentage on second row. This make is much easier to compare between corpuses and see upfront what the amount of overlapping and unique words looks like.\n",
    "\n",
    "Now that we can have an initial understanding of the uniqueness of each corpus, we want to see the detail, so this part is where I use the list proceduced by `intersection()` and `difference()` and retreive its frequency distribution from the original list, this is done by list comprehension which goes through each corpuses' corrsponding frequency distribusion list, then arranged from most common to least common. This is then finally organised and put into a dataframe with `Billboard`, `oardway`, `Opera`, `Billboard-Opera common`, `Billboard-Broadway common`, `Broadway-Opera common` as headers. This again, is then exported as markdown tables for ease of use and legibility.\n",
    "\n",
    "At this point, we have gained a lot of understanding on these corpuses, but I wanted to compare to how unique and/or common the \"unique\" words are of each corpus to the English language, so I used the word frequency list from the **Gutenburg** project. Again list comprehension is used here to compare and store in a new list under these criteria: 1. character counts, and later on, 2. the ranking on the Gutenburg list.\n",
    "\n",
    "This list is initially graphed using boxplot with two different versions: one with unknown rankings set as lowest rank (100,000); and one with unknown ranked words removed. This allows us to see two very different boxplots that can broaden the understanding of these words.\n",
    "\n",
    "Finally, these unique words under the previously mentioned criteria are exported into two different dataframs, with one with unknown ranking words, and the other one with words above a specified ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program\n",
    "### Part 1: Data loadout + pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading necessary imports and other vars, also check if all imports are installed, if not, install them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib.util\n",
    "import subprocess\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "def check_install(package):\n",
    "    spec = importlib.util.find_spec(package)\n",
    "    if spec is None:\n",
    "        print(f\"Installing {package}\")\n",
    "        install(package)\n",
    "    else:\n",
    "        print(f\"{package} is already installed\")\n",
    "\n",
    "libs = ['nltk', 'numpy', 'pandas', 'matplotlib', 'matplotlib_venn']\n",
    "for lib in libs:\n",
    "    check_install(lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "import corpus_loader\n",
    "\n",
    "CORPUS_DIR = '../corpus/'\n",
    "BILLBOARD = 'Billboard/'\n",
    "BROADWAY = 'Broadway/'\n",
    "OPERA = 'Opera/'\n",
    "\n",
    "COMBINED = 'Combined/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Billboard_NLTK, Billboard_Dict, Billboard_Df = corpus_loader.load_corpus(CORPUS_DIR+BILLBOARD+'2024/')\n",
    "\n",
    "Braodway_NLTK, Broadway_Dict, Broadway_Df = corpus_loader.load_corpus(CORPUS_DIR+BROADWAY+COMBINED)\n",
    "Lion_King_NLTK, Lion_King_Dict, Lion_King_Df = corpus_loader.load_corpus(CORPUS_DIR+BROADWAY+'Lion King/')\n",
    "Hamilton_NLTK, Hamilton_Dict, Hamilton_Df = corpus_loader.load_corpus(CORPUS_DIR+BROADWAY+'Hamilton/')\n",
    "Book_of_Mormon_NLTK, Book_of_Mormon_Dict, Book_of_Mormon_Df = corpus_loader.load_corpus(CORPUS_DIR+BROADWAY+'Book of Mormon/')\n",
    "\n",
    "Opera_NLTK, Opera_Dict, Opera_Df = corpus_loader.load_corpus(CORPUS_DIR+OPERA+COMBINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Billboard\")\n",
    "print(Billboard_NLTK.fileids())\n",
    "print(Billboard_NLTK.words())\n",
    "print(f'corpus total word length: {len(Billboard_NLTK.words())}')\n",
    "\n",
    "print(\"\\nBroadway\")\n",
    "print(Braodway_NLTK.fileids())\n",
    "print(Braodway_NLTK.words())\n",
    "print(f'corpus total word length: {len(Braodway_NLTK.words())}')\n",
    "\n",
    "print(\"\\nOpera\")\n",
    "print(Opera_NLTK.fileids())\n",
    "print(Opera_NLTK.words())\n",
    "print(f'corpus total word length: {len(Opera_NLTK.words())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the word frequency list in from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_list = corpus_loader.load_text(CORPUS_DIR+'word_frequency.txt').split('\\n')\n",
    "\n",
    "word_frequency_dict = {}\n",
    "rank = '1 - 1'\n",
    "for line in word_frequency_list:\n",
    "    if line == '':\n",
    "        continue\n",
    "    if line[0].isnumeric():\n",
    "        rank = line\n",
    "    else:\n",
    "        words = line.split(' ')\n",
    "        word_frequency_dict.update({word: rank for word in words})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom word processor to process the text and return all variance of the processing outcome, this is so I can compare the different processing methods and make sure all processing methods are working correctly and as expected to all text in the corpus and across different corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "class WordProcessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.regex_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.tokenizer = word_tokenize\n",
    "        \n",
    "    def process_text(self, text) -> dict:\n",
    "        \"\"\"\n",
    "        Process the text and return the tokenized, splitted, tokenized alphanumeric, \n",
    "        regex tokenized, regex tokenized without stop words, regex tokenized lematized, \n",
    "        regex tokenized without stop words and lematized\n",
    "\n",
    "        Args:\n",
    "            text (str): Text to be processed\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing all the processed text\n",
    "        \"\"\"\n",
    "        # processing methods\n",
    "        tokenised = self.tokenizer(text)\n",
    "        tokenised_upperremoved_alphaonly = [word.lower() for word in tokenised if word.isalpha() and word.upper() != word]\n",
    "        tk = [self.lemmatizer.lemmatize(word) for word in tokenised_upperremoved_alphaonly] # tokenized alphanumeric lematized\n",
    "        tk_sw = [word for word in tk if word not in self.stop_words] # tokenized without stop words\n",
    "\n",
    "\n",
    "        # collect frequency distribution of rtk_lm and rtk_sw_lm\n",
    "        fdist_tk = FreqDist(tk)\n",
    "        fdist_tk_sw = FreqDist(tk_sw)\n",
    "\n",
    "        # return the processed text\n",
    "        return { # tokens\n",
    "            'tk': tk,       # tokenized alphanumeric; full upper case (\"character hints\") removed; lematized\n",
    "            'tk_sw': tk_sw, # tokenized alphanumeric; full upper case (\"character hints\") removed; lematized; stop words removed\n",
    "        }, { # frequency distributions\n",
    "            'tk': fdist_tk,\n",
    "            'tk_sw': fdist_tk_sw,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the token and type frequency distribution of the text\n",
    "word_processor = WordProcessor()\n",
    "Billboard_tokens, Billboard_fdist = word_processor.process_text(' '.join(Billboard_NLTK.words()))\n",
    "Broadway_tokens, Broadway_fdist = word_processor.process_text(' '.join(Braodway_NLTK.words()))\n",
    "Opera_tokens, Opera_fdist = word_processor.process_text(' '.join(Opera_NLTK.words()))\n",
    "\n",
    "TOKENS = 'tk'\n",
    "TOKEN_SW = 'tk_sw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: basic data metrics and visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting an initial look at the basic metrics of the corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'corpus': ['Billboard', 'Broadway', 'Opera'],\n",
    "    'total words': [len(Billboard_NLTK.words()), len(Braodway_NLTK.words()), len(Opera_NLTK.words())],\n",
    "    'unique words': [len(Billboard_fdist[TOKENS]), len(Broadway_fdist[TOKENS]), len(Opera_fdist[TOKENS])],\n",
    "    'uw lexical diversity': [len(Billboard_fdist[TOKENS])/len(Billboard_tokens[TOKENS]), \n",
    "                             len(Broadway_fdist[TOKENS])/len(Broadway_tokens[TOKENS]), \n",
    "                             len(Opera_fdist[TOKENS])/len(Opera_tokens[TOKENS])],\n",
    "    'unique words (stop words removed)': [len(Billboard_fdist[TOKEN_SW]), len(Broadway_fdist[TOKEN_SW]), len(Opera_fdist[TOKEN_SW])],\n",
    "    'uw lexical diversity (stop words removed)': [len(Billboard_fdist[TOKEN_SW])/len(Billboard_tokens[TOKEN_SW]), \n",
    "                                                  len(Broadway_fdist[TOKEN_SW])/len(Broadway_tokens[TOKEN_SW]), \n",
    "                                                  len(Opera_fdist[TOKEN_SW])/len(Opera_tokens[TOKEN_SW])],\n",
    "})\n",
    "\n",
    "print(df.to_markdown())\n",
    "\n",
    "print(f\"\\nBillboard text\")\n",
    "print(Billboard_fdist[TOKENS])\n",
    "print(Billboard_tokens[TOKENS])\n",
    "print(Billboard_fdist[TOKEN_SW])\n",
    "print(Billboard_tokens[TOKEN_SW])\n",
    "\n",
    "print(f\"\\nBroadway text\")\n",
    "print(Broadway_fdist[TOKENS])\n",
    "print(Broadway_tokens[TOKENS])\n",
    "print(Broadway_fdist[TOKEN_SW])\n",
    "print(Broadway_tokens[TOKEN_SW])\n",
    "\n",
    "print(f\"\\nOpera text\")\n",
    "print(Opera_fdist[TOKENS])\n",
    "print(Opera_tokens[TOKENS])\n",
    "print(Opera_fdist[TOKEN_SW])\n",
    "print(Opera_tokens[TOKEN_SW])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot out the most frequent words, with one having stop words and one without, this is to have a better visualisaion and to initially scan for any interesting words or bugs (if present) to be proecessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "show_top = 80\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "axs[0].set_title('Billboard')\n",
    "axs[0].plot(*zip(*Billboard_fdist[TOKENS].most_common(show_top)))\n",
    "\n",
    "axs[1].set_title('Broadway')\n",
    "axs[1].plot(*zip(*Broadway_fdist[TOKENS].most_common(show_top)))\n",
    "\n",
    "axs[2].set_title('Opera')\n",
    "axs[2].plot(*zip(*Opera_fdist[TOKENS].most_common(show_top)))\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].set_ylabel('Frequency')\n",
    "    axs[i].xaxis.set_major_locator(FixedLocator(axs[i].get_xticks()))\n",
    "    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=90)\n",
    "    axs[i].grid()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "axs[0].set_title('Billboard')\n",
    "axs[0].plot(*zip(*Billboard_fdist[TOKEN_SW].most_common(show_top)))\n",
    "\n",
    "axs[1].set_title('Broadway')\n",
    "axs[1].plot(*zip(*Broadway_fdist[TOKEN_SW].most_common(show_top)))\n",
    "\n",
    "axs[2].set_title('Opera')\n",
    "axs[2].plot(*zip(*Opera_fdist[TOKEN_SW].most_common(show_top)))\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].set_ylabel('Frequency')\n",
    "    axs[i].xaxis.set_major_locator(FixedLocator(axs[i].get_xticks()))\n",
    "    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=90)\n",
    "    axs[i].grid()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Checking overlapping and unique words to each category and pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the pair-wise common words across the corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find overlapping words between the corpora\n",
    "Billboard_set = set(Billboard_fdist[TOKENS].keys())\n",
    "Broadway_set = set(Broadway_fdist[TOKENS].keys())\n",
    "Opera_set = set(Opera_fdist[TOKENS].keys())\n",
    "Billboard_sw_set = set(Billboard_fdist[TOKEN_SW].keys())\n",
    "Broadway_sw_set = set(Broadway_fdist[TOKEN_SW].keys())\n",
    "Opera_sw_set = set(Opera_fdist[TOKEN_SW].keys())\n",
    "\n",
    "print(f\"Billboard - Broadway: {Billboard_set.intersection(Broadway_set)}\")\n",
    "print(f\"Billboard - Opera: {Billboard_set.intersection(Opera_set)}\")\n",
    "print(f\"Broadway - Opera: {Broadway_set.intersection(Opera_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the unique and/or overlapping words into venn diagramms to see what it looks like and its percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "venn3([Billboard_set, Broadway_set, Opera_set], ('Billboard', 'Broadway', 'Opera'), ax=axs[0,0])\n",
    "axs[0,0].set_title('Unique words')\n",
    "\n",
    "venn3([Billboard_sw_set, Broadway_sw_set, Opera_sw_set], ('Billboard', 'Broadway', 'Opera'), ax=axs[0,1])\n",
    "axs[0,1].set_title('Unique words (stop words removed)')\n",
    "\n",
    "v1 = venn3([Billboard_set, Broadway_set, Opera_set], ('Billboard', 'Broadway', 'Opera'), ax=axs[1,0])\n",
    "axs[1,0].set_title('Unique words by percentage*')\n",
    "\n",
    "v2 = venn3([Billboard_sw_set, Broadway_sw_set, Opera_sw_set], ('Billboard', 'Broadway', 'Opera'), ax=axs[1,1])\n",
    "axs[1,1].set_title('Unique words (stop words removed)by percentage*')\n",
    "\n",
    "total = len(Billboard_set.union(Broadway_set.union(Opera_set)))\n",
    "v1.get_label_by_id('100').set_text(f'{len(Billboard_set.difference(Broadway_set.union(Opera_set)))/len(Billboard_set)*100:.2f}%')\n",
    "v1.get_label_by_id('010').set_text(f'{len(Broadway_set.difference(Billboard_set.union(Opera_set)))/len(Broadway_set)*100:.2f}%')\n",
    "v1.get_label_by_id('001').set_text(f'{len(Opera_set.difference(Billboard_set.union(Broadway_set)))/len(Opera_set)*100:.2f}%')\n",
    "v1.get_label_by_id('110').set_text(f'{len(Billboard_set.intersection(Broadway_set))/len(Billboard_set.union(Broadway_set))*100:.2f}%')\n",
    "v1.get_label_by_id('101').set_text(f'{len(Billboard_set.intersection(Opera_set))/len(Billboard_set.union(Opera_set))*100:.2f}%')\n",
    "v1.get_label_by_id('011').set_text(f'{len(Broadway_set.intersection(Opera_set))/len(Broadway_set.union(Opera_set))*100:.2f}%')\n",
    "v1.get_label_by_id('111').set_text(f'{len(Billboard_set.intersection(Broadway_set).intersection(Opera_set))/total*100:.2f}%')\n",
    "\n",
    "total = len(Billboard_sw_set.union(Broadway_sw_set.union(Opera_sw_set)))\n",
    "v2.get_label_by_id('100').set_text(f'{len(Billboard_sw_set.difference(Broadway_sw_set.union(Opera_sw_set)))/len(Billboard_sw_set)*100:.2f}%')\n",
    "v2.get_label_by_id('010').set_text(f'{len(Broadway_sw_set.difference(Billboard_sw_set.union(Opera_sw_set)))/len(Broadway_sw_set)*100:.2f}%')\n",
    "v2.get_label_by_id('001').set_text(f'{len(Opera_sw_set.difference(Billboard_sw_set.union(Broadway_sw_set)))/len(Opera_sw_set)*100:.2f}%')\n",
    "v2.get_label_by_id('110').set_text(f'{len(Billboard_sw_set.intersection(Broadway_sw_set))/len(Billboard_sw_set.union(Broadway_sw_set))*100:.2f}%')\n",
    "v2.get_label_by_id('101').set_text(f'{len(Billboard_sw_set.intersection(Opera_sw_set))/len(Billboard_sw_set.union(Opera_sw_set))*100:.2f}%')\n",
    "v2.get_label_by_id('011').set_text(f'{len(Broadway_sw_set.intersection(Opera_sw_set))/len(Broadway_sw_set.union(Opera_sw_set))*100:.2f}%')\n",
    "v2.get_label_by_id('111').set_text(f'{len(Billboard_sw_set.intersection(Broadway_sw_set).intersection(Opera_sw_set))/total*100:.2f}%')\n",
    "\n",
    "# add a note to the diagram\n",
    "fig.text(0.5, 0.03, '*percentage calculated with overlapping sets only', ha='center')\n",
    "fig.text(0.5, 0.01, 'e.g. yellow region = percentage of Billboard overlapping Broadway', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the 2-venn diagram to see the pair-wise relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn2\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 7))\n",
    "\n",
    "axs[0,1].set_title('Unique words (stop words removed)')\n",
    "venn2([Billboard_sw_set, Broadway_sw_set], ('Billboard', 'Broadway'), ax=axs[0,0])\n",
    "venn2([Billboard_sw_set, Opera_sw_set], ('Billboard', 'Opera'), ax=axs[0,1])\n",
    "venn2([Broadway_sw_set, Opera_sw_set], ('Broadway', 'Opera'), ax=axs[0,2])\n",
    "\n",
    "axs[1,1].set_title('Unique words by percentage')\n",
    "v1 = venn2([Billboard_sw_set, Broadway_sw_set], ('Billboard', 'Broadway'), ax=axs[1,0])\n",
    "v2 = venn2([Billboard_sw_set, Opera_sw_set], ('Billboard', 'Opera'), ax=axs[1,1])\n",
    "v3 = venn2([Broadway_sw_set, Opera_sw_set], ('Broadway', 'Opera'), ax=axs[1,2])\n",
    "\n",
    "total = len(Billboard_sw_set.union(Broadway_sw_set))\n",
    "v1.get_label_by_id('10').set_text(f'{len(Billboard_sw_set.difference(Broadway_sw_set))/len(Billboard_sw_set)*100:.2f}%')\n",
    "v1.get_label_by_id('01').set_text(f'{len(Broadway_sw_set.difference(Billboard_sw_set))/len(Broadway_sw_set)*100:.2f}%')\n",
    "v1.get_label_by_id('11').set_text(f'{len(Billboard_sw_set.intersection(Broadway_sw_set))/total*100:.2f}%')\n",
    "\n",
    "total = len(Billboard_sw_set.union(Opera_sw_set))\n",
    "v2.get_label_by_id('10').set_text(f'{len(Billboard_sw_set.difference(Opera_sw_set))/len(Billboard_sw_set)*100:.2f}%')\n",
    "v2.get_label_by_id('01').set_text(f'{len(Opera_sw_set.difference(Billboard_sw_set))/len(Opera_sw_set)*100:.2f}%')\n",
    "v2.get_label_by_id('11').set_text(f'{len(Billboard_sw_set.intersection(Opera_sw_set))/total*100:.2f}%')\n",
    "\n",
    "total = len(Broadway_sw_set.union(Opera_sw_set))\n",
    "v3.get_label_by_id('10').set_text(f'{len(Broadway_sw_set.difference(Opera_sw_set))/len(Broadway_sw_set)*100:.2f}%')\n",
    "v3.get_label_by_id('01').set_text(f'{len(Opera_sw_set.difference(Broadway_sw_set))/len(Opera_sw_set)*100:.2f}%')\n",
    "v3.get_label_by_id('11').set_text(f'{len(Broadway_sw_set.intersection(Opera_sw_set))/total*100:.2f}%')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter and find each lists' frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most common words in each paired overlapping sets\n",
    "billboard_braodway_common = Billboard_sw_set.intersection(Broadway_sw_set)\n",
    "billboard_opera_common = Billboard_sw_set.intersection(Opera_sw_set)\n",
    "broadway_opera_common = Broadway_sw_set.intersection(Opera_sw_set)\n",
    "\n",
    "billboard_braodway_common = {word: Billboard_fdist[TOKEN_SW][word] + Broadway_fdist[TOKEN_SW][word] for word in billboard_braodway_common}\n",
    "billboard_opera_common = {word: Billboard_fdist[TOKEN_SW][word] + Opera_fdist[TOKEN_SW][word] for word in billboard_opera_common}\n",
    "broadway_opera_common = {word: Broadway_fdist[TOKEN_SW][word] + Opera_fdist[TOKEN_SW][word] for word in broadway_opera_common}\n",
    "\n",
    "billboard_braodway_common = {k: v for k, v in sorted(billboard_braodway_common.items(), key=lambda item: item[1], reverse=True)}\n",
    "billboard_opera_common = {k: v for k, v in sorted(billboard_opera_common.items(), key=lambda item: item[1], reverse=True)}\n",
    "broadway_opera_common = {k: v for k, v in sorted(broadway_opera_common.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "common_words = {word: Billboard_fdist[TOKEN_SW][word] + Broadway_fdist[TOKEN_SW][word] + Opera_fdist[TOKEN_SW][word] for word in billboard_braodway_common}\n",
    "common_words = {k: v for k, v in sorted(common_words.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "print(f\"Billboard - Broadway common words: {billboard_braodway_common}\")\n",
    "print(f\"Billboard - Opera common words: {billboard_opera_common}\")\n",
    "print(f\"Broadway - Opera common words: {broadway_opera_common}\")\n",
    "\n",
    "print(f\"Common words: {common_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique words in each corpus\n",
    "Billboard_unique = Billboard_set.difference(Broadway_set.union(Opera_set))\n",
    "Broadway_unique = Broadway_set.difference(Billboard_set.union(Opera_set))\n",
    "Opera_unique = Opera_set.difference(Billboard_set.union(Broadway_set))\n",
    "\n",
    "Billboard_unique = {word: Billboard_fdist[TOKEN_SW][word] for word in Billboard_unique}\n",
    "Broadway_unique = {word: Broadway_fdist[TOKEN_SW][word] for word in Broadway_unique}\n",
    "Opera_unique = {word: Opera_fdist[TOKEN_SW][word] for word in Opera_unique}\n",
    "\n",
    "Billboard_unique = {k: v for k, v in sorted(Billboard_unique.items(), key=lambda item: item[1], reverse=True)}\n",
    "Broadway_unique = {k: v for k, v in sorted(Broadway_unique.items(), key=lambda item: item[1], reverse=True)}\n",
    "Opera_unique = {k: v for k, v in sorted(Opera_unique.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "print(f\"Billboard unique words: {Billboard_unique}\")\n",
    "print(f\"Broadway unique words: {Broadway_unique}\")\n",
    "print(f\"Opera unique words: {Opera_unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "organise the data into dataframes for eaiser usage and legibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_top = 50\n",
    "\n",
    "df = pd.DataFrame(columns=['Billboard', 'Boardway', 'Opera', 'Billboard-Opera common', 'Billboard-Broadway common', 'Broadway-Opera common'])\n",
    "\n",
    "df['Billboard'] = [word for word, _ in Billboard_unique.items()][:show_top]\n",
    "df['Boardway'] = [word for word, _ in Broadway_unique.items()][:show_top]\n",
    "df['Opera'] = [word for word, _ in Opera_unique.items()][:show_top]\n",
    "df['Billboard-Opera common'] = [word for word, _ in billboard_opera_common.items()][:show_top]\n",
    "df['Billboard-Broadway common'] = [word for word, _ in billboard_braodway_common.items()][:show_top]\n",
    "df['Broadway-Opera common'] = [word for word, _ in broadway_opera_common.items()][:show_top]\n",
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw boxplots on the ranking of unique words with set criterias, with the differences in unknown rankins' data manipulation to see what happenes and its effects of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = 10\n",
    "# make a boxplot of the word frequency rank\n",
    "Billboard_unique_rank = [word_frequency_dict.get(word, 0) for word in Billboard_unique if len(word) > characters]\n",
    "Broadway_unique_rank = [word_frequency_dict.get(word, 0) for word in Broadway_unique if len(word) > characters]\n",
    "Opera_unique_rank = [word_frequency_dict.get(word, 0) for word in Opera_unique if len(word) > characters]\n",
    "\n",
    "# change 0 to 100000 for words not in the word frequency list\n",
    "Billboard_unique_rank = [100000 if rank == 0 else int(rank.split(' ')[0]) for rank in Billboard_unique_rank]\n",
    "Broadway_unique_rank = [100000 if rank == 0 else int(rank.split(' ')[0]) for rank in Broadway_unique_rank]\n",
    "Opera_unique_rank = [100000 if rank == 0 else int(rank.split(' ')[0]) for rank in Opera_unique_rank]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "axs[0].boxplot(Billboard_unique_rank)\n",
    "axs[0].set_title('Billboard')\n",
    "axs[0].set_ylabel('Word Frequency Rank')\n",
    "\n",
    "axs[1].boxplot(Broadway_unique_rank)\n",
    "axs[1].set_title('Broadway')\n",
    "\n",
    "axs[2].boxplot(Opera_unique_rank)\n",
    "axs[2].set_title('Opera')\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].grid()\n",
    "    axs[i].set_ylim(1, 100000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = 10\n",
    "# make a boxplot of the word frequency rank\n",
    "Billboard_unique_rank = [word_frequency_dict.get(word, 0) for word in Billboard_unique if len(word) > characters]\n",
    "Broadway_unique_rank = [word_frequency_dict.get(word, 0) for word in Broadway_unique if len(word) > characters]\n",
    "Opera_unique_rank = [word_frequency_dict.get(word, 0) for word in Opera_unique if len(word) > characters]\n",
    "\n",
    "# remove 0, and also split the rank by space and take the first number as integer\n",
    "Billboard_unique_rank = [int(rank.split(' ')[0]) for rank in Billboard_unique_rank if rank != 0]\n",
    "Broadway_unique_rank = [int(rank.split(' ')[0]) for rank in Broadway_unique_rank if rank != 0]\n",
    "Opera_unique_rank = [int(rank.split(' ')[0]) for rank in Opera_unique_rank if rank != 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "axs[0].boxplot(Billboard_unique_rank)\n",
    "axs[0].set_title('Billboard')\n",
    "axs[0].set_ylabel('Word Frequency Rank')\n",
    "\n",
    "axs[1].boxplot(Broadway_unique_rank)\n",
    "axs[1].set_title('Broadway')\n",
    "\n",
    "axs[2].boxplot(Opera_unique_rank)\n",
    "axs[2].set_title('Opera')\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].grid()\n",
    "    axs[i].set_ylim(1, 100000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organise the most rare and complex words into dataframes to easily pick out interesting datas, sorted by known and unknown rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = 10\n",
    "desire_rank = 80000\n",
    "\n",
    "df = pd.DataFrame(columns=['Billboard', 'Billboard_rank', 'Broadway', 'Broadway_rank', 'Opera', 'Opera_rank'])\n",
    "\n",
    "i=0\n",
    "for word in Billboard_unique:\n",
    "    if len(word) > characters:\n",
    "        rank = word_frequency_dict.get(word, 0)\n",
    "        if rank !=0 and int(rank.split(' ')[0]) >= desire_rank:\n",
    "            df.loc[i, 'Billboard'] = word\n",
    "            df.loc[i, 'Billboard_rank'] = rank\n",
    "            i+=1\n",
    "\n",
    "i=0\n",
    "for word in Broadway_unique:\n",
    "    if len(word) > characters:\n",
    "        rank = word_frequency_dict.get(word, 0)\n",
    "        if rank !=0 and int(rank.split(' ')[0]) >= desire_rank:\n",
    "            df.loc[i, 'Broadway'] = word\n",
    "            df.loc[i, 'Broadway_rank'] = rank\n",
    "            i+=1\n",
    "\n",
    "i=0\n",
    "for word in Opera_unique:\n",
    "    if len(word) > characters:\n",
    "        rank = word_frequency_dict.get(word, 0)\n",
    "        if rank !=0 and int(rank.split(' ')[0]) >= desire_rank:\n",
    "            df.loc[i, 'Opera'] = word\n",
    "            df.loc[i, 'Opera_rank'] = rank\n",
    "            i+=1\n",
    "\n",
    "# replace nan with empty string\n",
    "df = df.fillna('')\n",
    "\n",
    "# print percentage of words meeting the criteria vs the total unique words of each corpus\n",
    "print(f\"Billboard: {len(df[df['Billboard'] != ''])/len(Billboard_unique)*100:.2f}%\")\n",
    "print(f\"Broadway:  {len(df[df['Broadway'] != ''])/len(Broadway_unique)*100:.2f}%\")\n",
    "print(f\"Opera:     {len(df[df['Opera'] != ''])/len(Opera_unique)*100:.2f}%\\n\")\n",
    "\n",
    "print(df[:50].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Billboard', 'Billboard_rank', 'Broadway', 'Broadway_rank', 'Opera', 'Opera_rank'])\n",
    "\n",
    "i=0\n",
    "for word in Billboard_unique:\n",
    "    if len(word) > characters:\n",
    "        rank = word_frequency_dict.get(word, 0)\n",
    "        if rank ==0:\n",
    "            df.loc[i, 'Billboard'] = word\n",
    "            df.loc[i, 'Billboard_rank'] = rank\n",
    "            i+=1\n",
    "\n",
    "i=0\n",
    "for word in Broadway_unique:\n",
    "    if len(word) > characters:\n",
    "        rank = word_frequency_dict.get(word, 0)\n",
    "        if rank ==0:\n",
    "            df.loc[i, 'Broadway'] = word\n",
    "            df.loc[i, 'Broadway_rank'] = rank\n",
    "            i+=1\n",
    "\n",
    "i=0\n",
    "for word in Opera_unique:\n",
    "    if len(word) > characters:\n",
    "        rank = word_frequency_dict.get(word, 0)\n",
    "        if rank ==0:\n",
    "            df.loc[i, 'Opera'] = word\n",
    "            df.loc[i, 'Opera_rank'] = rank\n",
    "            i+=1\n",
    "\n",
    "# replace nan with empty string\n",
    "df = df.fillna('')\n",
    "\n",
    "# print percentage of words meeting the criteria vs the total unique words of each corpus\n",
    "print(f\"Billboard: {len(df[df['Billboard'] != ''])/len(Billboard_unique)*100:.2f}%\")\n",
    "print(f\"Broadway:  {len(df[df['Broadway'] != ''])/len(Broadway_unique)*100:.2f}%\")\n",
    "print(f\"Opera:     {len(df[df['Opera'] != ''])/len(Opera_unique)*100:.2f}%\\n\")\n",
    "\n",
    "print(df[:50].to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and discussion\n",
    "\n",
    "The first initial observation of looking at the 3 corpuses, is that it appears Billboard and Broadway is actually closer together in terms of laxical diversity, as it reports within a 1% difference, where as Opera has a > 4~7% difference compared to either Billboard or Broadway. This can be referenced by the below chart:\n",
    "|    | corpus    |   total words |   unique words |   uw lexical diversity |   unique words (stop words removed) |   uw lexical diversity (stop words removed) |\n",
    "|---:|:----------|--------------:|---------------:|-----------------------:|------------------------------------:|--------------------------------------------:|\n",
    "|  0 | Billboard |         18741 |           1496 |               0.105553 |                                1373 |                                    0.199216 |\n",
    "|  1 | Broadway  |         61857 |           4591 |               0.109291 |                                4452 |                                    0.204633 |\n",
    "|  2 | Opera     |         31017 |           3109 |               0.145341 |                                2974 |                                    0.27727  |\n",
    "\n",
    "At this point, this seems to aling with my previous projects anlysis in that, current Billboard trending musics' lyrical complexity isn't that far differen from the genera of Broadway, and also proved (to an extend) that my previous suspection of opera having a bigger diference / impact is correct.\n",
    "\n",
    "From the charts displayed by the words frequency distribution chart, it is clear that stopwords **should** be removed to reduce noise and better comprehend the uniqueness of each corpus, however I still wanted to check it using different methods, which is then proven by the venn diagram. As it shows, the majority of the impact by removing the stopwords, are the center portion, which indicates it does not contribute to the uniqueness of the corpus, however it is still very pleasing to be able to see the visualisation in this way, enhacing the idea of why the stopwords list is a useful tool to use on hand. \n",
    "\n",
    "Using the venn diagram, we can eaisly see that Billboard holds the **least amount**  of unique words to itself, as it holds only 34.09% compared to Broadway's 61.90% and Opera's 49.19%. And from here is where the observation get really interesting, by comparing the 2-venn diagramm which shows the pair-wise relationship, we see that Billboard actually shares not many words with either Broadway or Opera, ranging from 16.78% ~ 17.65%, which is again within a 1 percent margin. However, the pair of Broadway - Opera shows a similiarity of 24.12%, which is almost a 7% increase from the other two pairs, sugesting that Billboard is more different from either Boradway or Opera, contridicting with the previous observation made via lexical diversity.\n",
    "![image of 3-venn diagram](venn1.png)\n",
    "![image of pairwise 2-venn diagram](venn2.png)\n",
    "\n",
    "Exploring further, the boxplots for complex words (character count > 10), it shows a very interesting trend: the mean, lower quartile, minimum of both Broadway and Opera are both ranked above (as in ranking, the plot is inverted, so the lower we go on y-axis the more common the words and are ranked higher) Billboard, however, the maximums and/or the outliers of both Broadway and Opera are also both ranked below the Billboard ranks. Through this graph, I can make the below assumption: the genere of Broadway and Opera both use **more** of the common words in the English language in **general**, meaning, most of the repeated language happens here. However, they both also use more of the more rare words compared to Billboard, suggesting a wider vocabulary using in its lyrics, though some may repeat a lot, hence the observation from laxicle diversity vs uniquess venn diagramm. The laxicle diversity explains the repeated usage of the more common words, and the venn diagram explains the usage of rare words present in the corpus.\n",
    "![image of boxplot](box2.png)\n",
    "\n",
    "Lastly, I tried to filter out the more common and less complex words, hoping to see somthing interesing, which returned very promissing results:   \n",
    "    * The unrankable words consists of mostly newer compound words, modern language, or arbitrary words that was created to fit the narritive, examples like: `motherfucker`, `transgender`, `enamabalascar`, `awaaaaaaaaaaaaaaaaay`.\n",
    "    * In the billboard list, all of the complex words are words I am fimilier with coming from a backgroun of English as a second language, however, the same cannot be said about the Broadway and Opera list. This hints at me that the latter two corpus use more complex words that are rare in the English language, compared to Billboard musics. Example words include but not limited to: `loathsomely`, `tribulation`, `whenceforth`, `ingenuitive`.\n",
    "    * In the ranked table, both Broadway and Opera corpus score double the percentage of usage of these unique, complex words compare to Billboard, with a score of 0.43%, 0.55%, and 0.21% ordering as listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Reflection\n",
    "\n",
    "In conclusion, from this research project I can say, modern Billboard music appears to be **less complex** compared to the genere of Boradway and Opera, however this observation is quite minute, accounting for only around or less than 1% of the whole text. Where as the overall lyrical space shares mostly the same usage of common and less complex words, though given larger sample space this observation may change, or even strengthen.\n",
    "\n",
    "During the devolpment of this program, I've encountered many different difficulties, one of which is the usage of word frequency list in the English languag. I searched on the interenet and found a few sourese, however most of them contains too little words (less than 5000) in the database, where onyl two of the data I found had around 100,000 samples, to which I've experimented with both, but ultimately found that the Gutenberg's list being the most comprehensive. One expremely important note here, the Gutenberg project was contudcted on 2005-2006, which is 9-10 years ago, which leads to many modern vocabulary left out, also as the language naturally evolves, the ranking will shift, and it is unsure of how it would effect the outcomes of the conclusion I've drawn upon based on this potentially outdated source.\n",
    "\n",
    "One of another major issue I encountered happened when exploring the boxplots, I decided to see the boxplot of the distribution of unique words with more complexity by evaluating the character counts, however there is a problem with words present not on the list of  Getenberg's word frequency ranking project, leading to many undefined ranking words. Initially, I decided to place these words as loweset rank, which is 100,000, however I believe this was not a good solution, as it greatly shifts the plot to the top side, skweing any usable data. I then tried to just remove the undefined ranking words, which showed a nice comparison of side by side box plots across the 3 genere. At this stage believe this is the best solution, again, there may be better options that I am currently unaware of that could resolve this is a manner that profits the strengh of this project more.\n",
    "![image of boxplot](box1.png)\n",
    "\n",
    "Lastly, the sample space I work with with the three corpus are as follows: \n",
    "|    | corpus    |   total words |\n",
    "|---:|:----------|---------------|\n",
    "|  0 | Billboard |         18741 |\n",
    "|  1 | Broadway  |         61857 |\n",
    "|  2 | Opera     |         31017 |\n",
    "\n",
    "This was due to me unable to find Billboard lists that is past top 40 that I believe to be stable/credible, and unable to find Opera shows that are solely in English that are more popular. I believe if I am able to expand both corpuses to around the same character counts as the Broadway corpus, this project will produce a more credible/sound results regardless of the outcomes, and if I were to expand on this project, this would be my first major point of interest to explore options on.\n",
    "\n",
    "Lastly, if I were to expand on this project, I would be aiming to dig deeper with the complex/rare words, perhaps into the semantics, or relations with other words, or any other options that may expand the filed of view on this matter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
